# ğŸš¨ IMPORTANT: BioKG-LoRA is Standalone

## Correction to Previous Documentation

**BioKG-LoRA does NOT reuse embeddings from GraphPath-VLM.**

### What Changed

**OLD** (Incorrect):
- âŒ "Reuse RotatE embeddings from GraphPath-VLM"
- âŒ "Already trained embeddings"
- âŒ "Fast turnaround (2-3 days)"

**NEW** (Correct):
- âœ… "Train RotatE embeddings from scratch"
- âœ… "Complete 4-stage pipeline"
- âœ… "Timeline: ~1 week from raw data to trained model"

---

## Complete Pipeline

### Stage 0: KG Construction âœ… IMPLEMENTED
- **Duration**: 1-2 days (CPU)
- **Script**: `scripts/stage0_build_kg.py`
- **What it does**: Downloads and integrates biological databases
- **Output**: `biological_kg.pt` (87K entities, 1.5M triples)

### Stage 1: RotatE Training âœ… IMPLEMENTED
- **Duration**: 2-3 days (1 GPU)
- **Script**: `scripts/stage1_train_rotate.py`
- **What it does**: Trains KG embeddings via link prediction
- **Output**: `entity_embeddings.pt` (87K Ã— 256-dim)

### Stage 2: Projection Training ğŸš§ TODO
- **Duration**: 2 hours (1 GPU)
- **Script**: `scripts/stage2_train_projection.py` (NOT YET CREATED)
- **What it does**: Aligns KG embeddings with LLM space
- **Output**: `projection_weights.pt`

### Stage 3: LoRA Fine-tuning ğŸš§ TODO
- **Duration**: 4-6 hours (1 GPU)
- **Script**: `scripts/stage3_train_lora.py` (NOT YET CREATED)
- **What it does**: Fine-tunes LLM with KG augmentation
- **Output**: `lora_adapter.pt`

---

## Why the Confusion?

The research document originally mentioned reusing embeddings as a **potential optimization** if you're running both BioKG-LoRA and GraphPath-VLM projects.

However, the **actual implementation** trains everything from scratch to make it:
- âœ… Self-contained
- âœ… Reproducible
- âœ… Independent
- âœ… Easier to understand

---

## What Works Right Now

### âœ… You Can Run Today

1. **Quick demo** (5 minutes):
   ```bash
   python scripts/quickstart.py
   ```

2. **Custom KG + RotatE training** (30 minutes):
   ```bash
   python scripts/stage0_build_kg.py --mode dummy --num_genes 500
   python scripts/stage1_train_rotate.py --kg_path data/kg/biological_kg.pt --entity2id_path data/kg/entity2id.json --num_epochs 20
   ```

3. **Test pipeline**:
   ```bash
   python tests/test_end_to_end.py
   ```

### ğŸš§ Still Need Implementation

- Stage 2 training script
- Stage 3 training script
- Real data source parsers
- QA dataset generation
- Full evaluation suite

---

## Timeline

### From Scratch (Default)

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  Complete BioKG-LoRA Pipeline Timeline                   â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Stage 0: Build KG             â”‚ 1-2 days  â”‚ CPU         â•‘
â•‘  Stage 1: Train RotatE         â”‚ 2-3 days  â”‚ 1 GPU       â•‘
â•‘  Stage 2: Train Projection     â”‚ 2 hours   â”‚ 1 GPU       â•‘
â•‘  Stage 3: LoRA Fine-tuning     â”‚ 4-6 hours â”‚ 1 GPU       â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  TOTAL                         â”‚ ~1 week   â”‚             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### If Sharing with GraphPath-VLM (Optional)

If you've already built the KG and trained RotatE for GraphPath-VLM:

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  Using Existing Embeddings (Optional)                    â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Stage 0: Build KG             â”‚ SKIP      â”‚ Already doneâ•‘
â•‘  Stage 1: Train RotatE         â”‚ SKIP      â”‚ Already doneâ•‘
â•‘  Stage 2: Train Projection     â”‚ 2 hours   â”‚ 1 GPU       â•‘
â•‘  Stage 3: LoRA Fine-tuning     â”‚ 4-6 hours â”‚ 1 GPU       â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  TOTAL                         â”‚ ~1 day    â”‚             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**But the default implementation trains from scratch!**

---

## Key Points

1. âœ… **Standalone by default** - No dependency on GraphPath-VLM
2. âœ… **Trains from scratch** - Complete 4-stage pipeline
3. âœ… **Stages 0-1 implemented** - KG + RotatE ready
4. ğŸš§ **Stages 2-3 TODO** - Need training scripts
5. ğŸ”„ **Can share optionally** - If running both projects

---

## Corrected Documentation

These files have been updated to reflect the standalone nature:
- âœ… `mkdocs/docs/research_biokg_lora.md` - Removed "reuse" language
- âœ… `biokg-lora/README.md` - Updated to 4-stage pipeline
- âœ… `biokg-lora/STANDALONE_CLARIFICATION.md` - New clarification doc
- âœ… `biokg-lora/README_CORRECTION.md` - This file

---

## Questions?

**Q: Why did the docs say "reuse"?**  
A: Early drafts mentioned it as an optimization. Implementation is standalone.

**Q: Can I still share embeddings?**  
A: Yes, if you want. But it's not required or assumed.

**Q: What's the recommended approach?**  
A: Train from scratch (default) for reproducibility.

**Q: How long from scratch?**  
A: ~1 week (mostly GPU time for Stage 1).

---

## Bottom Line

ğŸ¯ **BioKG-LoRA is a complete, self-contained project that trains all embeddings from scratch.**

No external dependencies. No assumptions. Just run the scripts in order.

âœ… Stages 0-1: **READY TO USE**  
ğŸš§ Stages 2-3: **NEED TRAINING SCRIPTS**
