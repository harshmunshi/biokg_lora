# RotatE Training Pipeline Diagram - Summary

## ğŸ“Š Visual Overview Added to Documentation

A comprehensive ASCII diagram has been added to **Section 7.1.0** of `research_biokg_lora.md` showing the complete RotatE training pipeline.

---

## ğŸ¯ What the Diagram Shows

### 1. INPUT: Knowledge Graph Triples

```
From Stage 0 â†’ biological_kg.pt
â”œâ”€ 87,452 entities
â”œâ”€ 1,458,203 triples
â””â”€ 15 relation types

Example triples:
  (Thbd, regulates, Coagulation_Cascade)
  (Bmp4, expressed_in, Kidney)
  (Fgfr2, causes, MP:0003350)  â† phenotype
```

**What goes in**: Raw KG triples from Stage 0 construction

---

### 2. TRAINING OBJECTIVE: Link Prediction

```
Task: Given (head, relation, ?), predict missing tail
      Given (?, relation, tail), predict missing head

Training Strategy:
â”œâ”€ 1 positive triple:  (Thbd, regulates, Coagulation_Cascade) âœ“
â””â”€ 128 negative triples: (Thbd, regulates, Kidney) âœ—
                         (Thbd, regulates, Bmp4) âœ—
                         ...
```

**What it learns**: Which entity completions are valid vs invalid

---

### 3. MODEL ARCHITECTURE: RotatE (5 Steps)

#### Step 1: Entity Embedding Lookup
```
Entity Embedding Table (87,452 Ã— 256)
  Thbd    â†’ [0.12, -0.34, ..., 0.56]
  Kidney  â†’ [0.67, -0.11, ..., 0.33]
  ...
```

#### Step 2: Represent as Complex Numbers
```
Split 256-dim â†’ 128 complex numbers
  h = [hâ‚€, hâ‚, ..., hâ‚â‚‚â‚‡]  where háµ¢ = aáµ¢ + báµ¢i
```

#### Step 3: Relation as Rotation
```
Relation Embedding Table (15 Ã— 128 angles)
  regulates â†’ [Î¸â‚€, Î¸â‚, ..., Î¸â‚â‚‚â‚‡]
  
Convert to unit circle: ráµ¢ = e^(iÎ¸áµ¢) = cos(Î¸áµ¢) + iÂ·sin(Î¸áµ¢)
```

#### Step 4: Rotate Head by Relation
```
Complex multiplication: h âˆ˜ r
  h_rotated = [hâ‚€Â·râ‚€, hâ‚Â·râ‚, ..., hâ‚â‚‚â‚‡Â·râ‚â‚‚â‚‡]
  
This rotates h in complex space!
```

#### Step 5: Compute Distance to Tail
```
Score = ||h âˆ˜ r - t||

Low score  â†’ h âˆ˜ r â‰ˆ t â†’ Triple is TRUE  âœ“
High score â†’ h âˆ˜ r â‰  t â†’ Triple is FALSE âœ—
```

---

### 4. TRAINING LOSS: Self-Adversarial Negative Sampling

```
For each positive triple (h, r, t):

1. Positive score: sâº = ||h âˆ˜ r - t||

2. Generate 128 negative samples:
   - 64 by corrupting head: (h', r, t)
   - 64 by corrupting tail: (h, r, t')

3. Negative scores: sâ» = [sâ‚â», sâ‚‚â», ..., sâ‚â‚‚â‚ˆâ»]

4. Self-adversarial weighting (focus on hard negatives):
   wáµ¢ = softmax(-Î±sáµ¢â»)  â† higher if sáµ¢â» is low (hard negative)

5. Margin-based loss:
   â„’ = -log Ïƒ(Î³ - sâº) - Î£áµ¢ wáµ¢Â·log Ïƒ(sáµ¢â» - Î³)
   
   where Î³ = 9.0 (margin)
```

**Key Innovation**: Hard negatives (low score, look plausible) get more weight during training â†’ forces model to learn fine-grained distinctions

---

### 5. OUTPUT: Trained Embeddings

```
After 500 epochs (~3 days on A100):

entity_embeddings.pt
â”œâ”€ Shape: (87,452, 256)
â”œâ”€ Size: 85 MB
â””â”€ Properties:
   â€¢ Similar entities close in space
   â€¢ Semantic relationships preserved
   â€¢ Ready for downstream tasks

relation_embeddings.pt
â”œâ”€ Shape: (15, 128)
â””â”€ Size: 8 KB

Performance:
â”œâ”€ MRR: 0.68
â”œâ”€ Hits@1: 52%  (correct entity in top 1)
â”œâ”€ Hits@3: 78%
â””â”€ Hits@10: 89%
```

---

## ğŸ§  What These Embeddings Capture

### Entity Embeddings Encode:

```
âœ“ Gene function (kinase, transcription factor, ...)
âœ“ Tissue specificity (kidney-expressed, liver, ...)
âœ“ Pathway membership (coagulation, apoptosis, ...)
âœ“ Phenotype associations (renal, cardiac, skeletal)
âœ“ Protein interactions (hub genes vs peripheral)
âœ“ Evolutionary relationships (gene families)
```

**Example Similarities**:
```
Thbd  â†” Proc   (both in coagulation cascade)
Bmp4  â†” Bmp7   (same gene family)
Kidney â†” Nephron (tissue hierarchy)
```

### Relation Embeddings Encode:

```
âœ“ Semantic relationship type
âœ“ Symmetric relations â†’ rotation by Ï€
âœ“ Antisymmetric relations â†’ unique rotation
âœ“ Inverse relations (causes â†” caused_by) â†’ r vs -r
âœ“ Composition (multi-hop reasoning) â†’ compose rotations
```

---

## ğŸ”— Connection to Stage 2

```
entity_embeddings.pt â†’ Projection Layer â†’ LLM token space
     (87K, 256)          (256 â†’ 4096)      (87K, 4096)
```

The projection layer (Stage 2) maps these biological embeddings into the LLM's token space, allowing the LLM to "see" and reason about the knowledge graph!

---

## ğŸ“ Why This Diagram Matters

### For Understanding:
- **Visual learners** can see the data flow
- **Step-by-step** breakdown of complex process
- **Concrete examples** at each stage

### For Implementation:
- Clear **input format** specification
- **Architecture details** for coding
- **Expected outputs** for validation

### For Research:
- **Training objective** clearly stated
- **Loss function** fully specified
- **Evaluation metrics** documented

---

## ğŸ“ Location in Documentation

**File**: `/mkdocs/docs/research_biokg_lora.md`

**Section**: 7.1.0 "Training Pipeline Overview" (NEW)

**Line**: ~775 (right before the RotatE architecture code)

**Size**: ~200 lines of ASCII art + explanations

---

## ğŸ¨ Diagram Features

### âœ… What's Included:

1. **Input specification** with example triples
2. **Training objective** (link prediction)
3. **5-step model architecture** with math
4. **Loss function** with formulas
5. **Output format** and metrics
6. **Semantic interpretation** of embeddings
7. **Connection to next stage**

### ğŸ“Š Visual Elements:

- **Boxes** for key components
- **Arrows** showing data flow
- **Examples** at each step
- **Math notation** for precision
- **Performance metrics**
- **Intuition** sections

---

## ğŸš€ Usage

### For Students/Researchers:
Read this diagram **before** diving into the code to understand:
- What RotatE does
- Why it works
- What to expect

### For Implementers:
Use this as a **specification** for:
- Input data format
- Model architecture
- Training loop
- Validation metrics

### For Reviewers:
Reference this to quickly understand:
- The approach
- The training objective
- Expected results

---

## âœ¨ Key Takeaways

### 1. Clear Data Flow
```
KG Triples â†’ RotatE Model â†’ Entity Embeddings â†’ Stage 2
```

### 2. Training is Link Prediction
```
Learn embeddings such that:
  True triples score LOW  (close in space)
  False triples score HIGH (far in space)
```

### 3. RotatE's Innovation
```
Relations = Rotations in complex space
This allows modeling:
  â€¢ Symmetry (rotation by Ï€)
  â€¢ Inversion (r vs -r)
  â€¢ Composition (multiply rotations)
```

### 4. Self-Adversarial Training
```
Hard negatives get more weight
â†’ Forces fine-grained learning
â†’ Better generalization
```

### 5. Biologically Meaningful
```
Embeddings capture:
  Gene function âœ“
  Tissue specificity âœ“
  Pathway relationships âœ“
  Phenotype associations âœ“
```

---

## ğŸ¯ Diagram Completeness

| Aspect | Covered? | Detail Level |
|--------|----------|--------------|
| Input format | âœ… | High - with examples |
| Training objective | âœ… | High - with task definition |
| Model architecture | âœ… | Very high - 5 steps with math |
| Loss function | âœ… | High - formula + intuition |
| Training strategy | âœ… | High - negative sampling |
| Output format | âœ… | High - shapes + metrics |
| Semantic meaning | âœ… | High - what's captured |
| Next steps | âœ… | Medium - connection to Stage 2 |

**Overall**: Publication-quality diagram ready for papers/presentations! ğŸ‰
